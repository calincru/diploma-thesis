\chapter{Evaluation}\label{chapter:eval}

In this chapter we summarize the key results we reached by running SymNet on
various models generated by our tool, \TOOL.  It is divided into two sections.
In the first one we argue that the models we build capture the packet
processing logic behind netfilter.  In the second one we use models built from
synthetic rules to evaluate the quality of our code generation approach
discussed in previous chapters.


\section{Acceptance tests}

Acceptance tests help answer the question \emph{"Does our model reflect the
semantics of netfilter?"}  Certainly we would rather not introduce
inconsistencies in a software system that is supposed to find bugs in another
one. In some formal verification systems the \emph{model - real system}
equivalence is enforced by construction.  However, this is not true for SymNet
for reasons described in \labelindexref{Section}{sec:symnet-sefl}, and, thus,
we need to run hand-crafted tests to confirm our expectations.

Driven by this insight from the very beginning, we have performed extensive
unit testing, focusing on specific components and/or behaviours, as well as
integration testing, when linking different parts together.  Our testing suite
contains more than \textbf{130 total tests} and achieves over \textbf{91\% code
coverage}~\cite{github-repo}.  This means that we have generated models that
cover most of our model generation corner cases.  However, this should not be
confused with \emph{generated code} coverage, which depends on input packet,
input port, as well as the interaction with the network as a whole, considering
that some of the networks elements that we model are stateful.

In the following paragraphs we show a small subset of the scenarios that we
tested against.

\paragraph{Simple NAT policy.}
We start with a very simple example to give an intuition of how these tests are
performed by including the Scala code too.

\labelindexref{Listing}{lst:unreachable-rule} shows the entire test.  Between
\textbf{lines 2-7} the nat/POSTROUTING chain is set up with two rules: one to
match an entire private network (\lstinline{-s 192.168.1.0/24}) and the other
one to only match a host in the  same network (\lstinline{-s 192.168.1.100}).
Between \textbf{lines 8-13} we run symbolic execution injecting a packet with
the source IP set to \lstinline{192.168.1.100} (line 12), starting from this
chain's input port (lines 10-11).  Between \textbf{lines 14-19} we define the
\emph{rewrite constrain} that we expect to take place.  Finally, at line 22 we
\textbf{express the policy} using the custom Scala matcher
\emph{containConstrain} that looks up the given constrain in the list of
successful paths output by symbolic execution.

\begin{listing}[H]
  \caption{An example of a NAT misconfiguration taken from a \emph{Local Area
  Networks} lecture quiz.  Notice that Scala already makes policy specification
  easy to understand and express owing to its relaxed syntax rules.}
  \label{lst:unreachable-rule}
  \sourcecode{scala}{assets/code/simple-nat.scala}
\end{listing}

This example is intended to show how a network administrator would express
policies and catch inconsistencies.  Indeed, this test fails because the
specified host IP matches the first rule in the table.  In fact, the second
rule will never be matched, indicating a misconfiguration which would hopefully
be found as a result of the policy failing.

\paragraph{NAT-ed connections.}
To take our previous SNAT configuration from a toy example that is intended to
give a glimpse at how simple policies can be expressed in code to a
fully-fledged, end-to-end one, we use a network topology as shown in
\labelindexref{Figure}{fig:source-nating}.

\begin{figure}[h]
  \centering
  \captionsetup{justification=centering}
  \includegraphics[scale=0.5]{assets/img/source-nating}
  \caption{Generic network configuration used to test various stateful
  functions.}
  \label{fig:source-nating}
\end{figure}

The \textbf{IP mirror} component is used to model reply packets.  All it does
is to swap the source and destination addresses (both IP and port).  Thus, by
connecting two of them through an intermediate device that is our
iptables-enhanced router, we obtain bi-directional communication between them.
This is in fact a pattern we often use when testing any kind of stateful
configuration.  It is not tied to NAT-ing in any way.

How do we specialize it for NAT testing?  Besides the rules that define our
model, the specific scenarios that we test are controlled by input packets and
input ports.  For instance:
\begin{itemize}
  \item To test that SNAT rules are \textbf{correctly applied}, we inject an
    initial packet that matches the rule we are interested in on the output
    port of \emph{IP mirror 1} and check if a packet with the rewritten source
    IP reaches the input port of \emph{IP mirror 2}.
  \item To test that \textbf{reverse} SNAT rules are correctly applied, we add
    one more policy rule to the previous example: a packet with the reverse
    source/destination IPs should reach the input port of \emph{IP mirror 1}.
  \item To test that the nat/POSTROUTING chain is traversed \textbf{only once}
    for a SNAT-ed connection, we add the following policy rule: the input port
    of this particular chain is traversed only once.  The special logic for
    skipping it otherwise is subjected to test here.
  \item To test that the same rule does not apply if the connection is
    initiated from the \textbf{opposite direction} (SNAT rules use source IP
    matches, in general), we inject a packet on the output port of \emph{IP
    mirror 2} and check that a packet with the rewritten source IP \textbf{does
    not} reach the input port of \emph{IP mirror 1}.
  \item To make the previous scenario work, we change our model by adding a
    DNAT rule in the nat/PREROUTING chain.  We make sure that all previous
    policies are still valid.
\end{itemize}

This is a non-exhaustive list but gives a good idea of the challenges behind
Network Address Translation testing.

\paragraph{Connection state switch.}
Another key feature that has been thoroughly tested is the transition between
connection states.

The network model we built to test it is shown in
\labelindexref{Figure}{fig:state-switch}.  It consists of our regular
iptables-enabled device (left) and an IP mirror element that is used to
generate reply packets, as previously described (right).  We simulate traffic
from our device to the server by injecting a concrete packet on the output port
of the local process component.  As discussed at the end of
\labelindexref{Chapter}{chapter:model}, we expose this port as part of our
model interface precisely for testing purposes like this one
(\labelindexref{Figure}{fig:local-process-out}).

A new metadata field is stored for each packet flow to track its connection
state, intuitively called \textbf{ctstate}.  The first time a packet belonging
to a connection is seen, its \emph{ctstate} is set to \NEW (transition
\textbf{1 -> 2}).  Once a packet in the \textbf{reverse} direction is
processed, the state becomes \ESTABLISHED (transition \textbf{3 -> 4}).  Our
policy in this case can be stated such as: \emph{there must be a packet flow
that reaches the local process and its ctstate field is ESTABLISHED}.

\begin{figure}[h]
  \centering
  \captionsetup{justification=centering}
  \includegraphics[scale=0.5]{assets/img/state-switch}
  \caption[Network model built to test connection tracking.]{Network model
  built to test connection tracking. Fields in bold font are the ones modified
  in the last step.}
  \label{fig:state-switch}
\end{figure}

\paragraph{Untracked connection.}
As a follow-up scenario for the previous one, we also ensured that marking
certain traffic as \UNTRACKED in the \emph{raw} table leads to no state
transitions.

\bigskip

An alternative approach to bypassing the \emph{model - real system} equivalence
problem that we have yet to experiment with is \emph{black-box testing}.  It
requires establishing a(n) (usually large) list of input packets and injecting
them both in the real system and in its model constructed with \TOOL.  Then,
monitor output packets in the real iptables-enabled device and try to match
them against the exhaustive list of (symbolic) packet flows output by SymNet.
If for some input packet there is no symbolic flow that matches the observed
output packet, the equivalence is denied.  The reversed implication is
obviously not necessarily true, but it increases our \emph{belief} that the
implementation is correct.


\section{Performance tests}

We use performance tests to measure verification time on synthetic models
assumed to be correct.  What this means is that they are automatically
generated based on some parameters (e.g. chain size, chains to populate, etc),
and also pass our validation procedures.  They can be grouped in two classes by
the main dimension that gets varied across each suite, and are further
discussed in the following paragraphs.

\paragraph{Chain sizes.} These tests runs symbolic execution on multiple models
by varying the number of rules in a chain.  We chose to test against the
filter/FORWARD chain, both because of its flexibility as all matches are
allowed here (e.g.  it is the only chain where \lstinline{--out-interface} and
\lstinline{--in-interface} can be used simultaneously), but also because
iptables is primarily used to implement software firewalls which make heavy use
of this chain.

The main results are shown in \labelindexref{Figure}{fig:rules-time}.  The
\textbf{x axis} is the log-scaled number of rules, while the \textbf{y axis}
represents verification time in seconds.  We perform this test for four
different configurations.  The maximum number of runs is six, but it is lower
for some of them due to memory constraints.  Also, each test is run on multiple
generated configurations and the average time is computed to ensure robustness
of results.

We started with a configuration made up solely from the aforementioned chain,
and using a purely symbolic packet to bootstrap symbolic execution. The result
is summarized by the black plot.  It features a linear increase on more than a
half of the interval, with an upper bound of 20 seconds.  This corresponds to a
logarithmic growth with regards to the number of rules.  Towards the other end
of the interval (200 to 1000 rules) there is a quick upsurge in its slope, but
it still remains under the one minute threshold.

The black and blue plots show similar traits, being roughly translated on the
\emph{y axis} as a result of introducing a few more rules in other chains
(nat/PREROUTING, mangle/PREROUTING). Essentially what happens is that instead
of starting with a \textbf{single purely} symbolic packet as above, a
\textbf{few constrained} packets reach the filter/FORWARD chain due to the
traversal of the previous ones.

Finally, the magenta colored plot corresponds to a scenario that mimics the
first one, but it differs in that that it bootstraps symbolic execution with a
\textbf{targeted} packet (a packet with a concrete destination IP address). The
runtime gain is dramatic, which shows the importance of having a policy drive
verification rather than exploring all possible execution outcomes.

\begin{minipage}[t]{.5\textwidth}
  \centering

  \pgfplotstableread{assets/data/filter-rules.dat}{\rules}
  \pgfplotstableread{assets/data/filter-nat2-rules.dat}{\rulees}
  \pgfplotstableread{assets/data/filter-nat4-rules.dat}{\ruleeees}
  \pgfplotstableread{assets/data/filter-rules-targeted.dat}{\ruls}

  \begin{tikzpicture}[scale=0.9]
    \begin{axis}[
        xmode=log,
        ymin=0,
        ymax=100,
        xlabel=Rules,
        ylabel=Time (s),
        ylabel near ticks,
        legend pos=north west,
      ]
      \addplot[mark=*] table [x={rules}, y={time}] {\rules};
      \addlegendentry{filter};

      \addplot[mark=square*,color=red] table [x={rules}, y={time}] {\rulees};
      \addlegendentry{filter+nat(2)};

      \addplot[mark=diamond*,color=blue] table [x={rules}, y={time}] {\ruleeees};
      \addlegendentry{filter+nat(2)+mangle(4)};

      \addplot[mark=triangle*,color=magenta] table [x={rules}, y={time}] {\ruls};
      \addlegendentry{filter (targeted)};
    \end{axis}
  \end{tikzpicture}

  \captionsetup{justification=centering}
  \captionof{figure}{Verification time by chain size.}
  \label{fig:rules-time}
\end{minipage}
~
\begin{minipage}[t]{.5\textwidth}
  \centering

  \pgfplotstableread{assets/data/network-depth10.dat}{\depthone}
  \pgfplotstableread{assets/data/network-depth20.dat}{\depthtwo}
  \pgfplotstableread{assets/data/network-depth50.dat}{\depthfive}

  \begin{tikzpicture}[scale=0.9]
    \begin{axis}[
        ymin=0,
        ymax=100,
        xlabel=Network depth,
        yticklabels={,,},
        legend pos=north west,
      ]
      \addplot[mark=*,color=black] table [x={depth}, y={time}] {\depthone};
      \addlegendentry{10 rules};

      \addplot[mark=square*,color=red] table [x={depth}, y={time}] {\depthtwo};
      \addlegendentry{20 rules};

      \addplot[mark=triangle*,color=blue] table [x={depth}, y={time}] {\depthfive};
      \addlegendentry{50 rules};
    \end{axis}
  \end{tikzpicture}

  \captionsetup{justification=centering}
  \captionof{figure}{Verification time by network depth.}
  \label{fig:depth-time}
\end{minipage}

\paragraph{Network depth.}  Besides how fast a chain of rules can be traversed,
it is arguably even more important how the result of this symbolic execution
affects the network as a whole.

The less constrained the input packet and the more complex the logic abstracted
by a device, the more packet flows might result.  This is true in general, but
it is particularly worrisome for iptables-enabled devices due to the
arbitrarily complicated behaviour that they can express.  That being said, all
packet flows that leave such a device can be materialized (i.e. can result from
a specific input packet).  This means that we do not generate unnecessary
packets outside the scope of a device.  Therefore, to avoid the multiplicative
effect when connecting multiple such devices, our only chance is to inject a
more constrained packet.

The results presented in \labelindexref{Figure}{fig:depth-time} show
verification time for networks built by chaining multiple iptables-enhanced
routers.  As motivated in the previous paragraph, we set the destination IP
address to a concrete value to reduce the number of outbound packets.  We
connected between 2 and 10 devices in three different configurations given by
the number of rules in their filter/FORWARD chains.

As expected, the number of chained devices decreases due to out of memory
errors as we increase the chain size.  If for 10 rules per chain we can verify
networks with depths up to 10 in approximately 30 seconds, when testing for the
case of 50 rules per chain, the maximum depth drops to 6 and takes up to 100
seconds before consuming all 32GB of RAM.

\bigskip

In addition to the script-generated configurations, we have also run SymNet on
a model of a Neutron L3 agent\footnote{It is essentially a router that uses the
Linux IP stack and iptables in an isolated \emph{network namespace}.  This in
turn is the Linux terminology for the \emph{Virtual Routing and Forwarding}
(VRF)\abbrev{VRF}{Virtual Routing and Forwarding} concept that takes the idea
of Virtual Local Area Networks (VLANs)\abbrev{VLAN}{Virtual Local Area Network}
from the L2 switching world to the L3 world.} from a real OpenStack deployment.
Its complete iptables dump is included in
\labelindexref{Appendix}{app:qrouter}.

Exploring all possible execution paths by injecting a pure symbolic packet on
one of its input ports takes around \textbf{25 seconds} and \textbf{6GB} of
RAM.  This generates 1345 packet flows.  Only 74 of them are successful.  Out
of all failed ones only 6 are explicitly dropped.  The remaining ones are
packets caused by internal logic of our model that cannot be materialized.
Since there are only two rules that might DROP packets, it becomes manageable
to inspect those packets by hand.  Moreover, if we constrain just the
destination IP address of the injected packet to a concrete value, symbolic
execution time drops to \textbf{5 seconds}.
